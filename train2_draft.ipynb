{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Painting Classifier by Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, RandomFlip, RandomZoom, RandomRotation, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all_data_info.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing some of the art "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_folder = \"/Users/aryan/Desktop/train_3\"\n",
    "\n",
    "# create a figure with 2 rows and 3 columns of subplots\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "\n",
    "# loop through the first 6 images\n",
    "for i in range(6):\n",
    "    # get the image path and read the image\n",
    "    img_name = os.listdir(train_1_folder)[i]\n",
    "    img_path = os.path.join(train_1_folder, img_name)\n",
    "    img = plt.imread(img_path)\n",
    "    \n",
    "    # find the corresponding row in the DataFrame\n",
    "    row = df[df[\"new_filename\"] == img_name].iloc[0]\n",
    "    \n",
    "    # get the artist, title, and style information\n",
    "    artist = row[\"artist\"]\n",
    "    title = row[\"title\"]\n",
    "    style = row[\"style\"]\n",
    "    \n",
    "    # plot the image in the appropriate subplot\n",
    "    row_index, col_index = divmod(i, 3)\n",
    "    axs[row_index, col_index].imshow(img)\n",
    "    axs[row_index, col_index].set_title(style)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident we have a ton of data to work with. Let's take a deeper look into the Style column since we will be working with it extensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"style\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge imbalance in the data here. We have 5 instances of artworks that fall under the spectralism category and have more than 10,000 impressionism style paintings. We would be naive to train our model on the dataset as is. We need to filter out styles that do not have enough samples and also those that have too many. Here, the choice for the threshold is really arbitrary and we decided to work with art styles with 2,000 samples. This means that styles like realism with several thousands of paintings need to be cut down to as close to 2,000 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"style\"].value_counts()[:15]\n",
    "\n",
    "sns.barplot(x=df[\"style\"].value_counts()[:15].values, y=df[\"style\"].value_counts()[:15].index,\n",
    "            color=\"r\")\n",
    "plt.xlabel(\"Number of paintings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the 15 most populated styles in our dataset, we see that Neoclassicism is just above our threshold with 2,038 samples. So, the next step is to cut down all samples above our threshold to meet our requirements for an unbiased dataset. To do this we will create a function to loop over the overpopulated styles, find the difference between them and the count for our Neoclassicism style, and reduce it to match our minimum limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overpop_styles = df[\"style\"].value_counts()[:10].index.tolist()\n",
    "overpop_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_balancer(style_arr, df):\n",
    "    \"\"\"\n",
    "    Reduce the number of samples for each overpopulated style in the given DataFrame to match the count of the\n",
    "    least represented style, and return a new DataFrame with the reduced number of samples for each style.\n",
    "\n",
    "    Parameters:\n",
    "        style_arr (list of str): A list of style names to balance.\n",
    "        df (pandas.DataFrame): The DataFrame containing the samples to balance.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with the reduced number of samples for each style.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the minimum number of samples for any style in the DataFrame\n",
    "    min_val = df[\"style\"].value_counts()[13] # assuming \"Neoclassicism\" is at index 13\n",
    "\n",
    "    # Initialize an empty DataFrame variable to hold the balanced samples for all styles\n",
    "    empty_df_var = None\n",
    "\n",
    "    # Loop through each style in the style_arr list\n",
    "    for _style in style_arr:\n",
    "        # Create a boolean mask to select only the samples with the current style\n",
    "        style_mask = df[\"style\"] == _style\n",
    "\n",
    "        # Create a new DataFrame with only the samples with the current style\n",
    "        style_df = df[style_mask]\n",
    "\n",
    "        # Calculate the total number of samples for the current style\n",
    "        tot = len(style_df.index)\n",
    "\n",
    "        # Calculate the number of samples to remove to match the minimum count\n",
    "        rmv = tot - min_val\n",
    "\n",
    "        # Remove the excess samples from the current style DataFrame\n",
    "        style_df.drop(style_df.index[range(rmv)], inplace=True)\n",
    "\n",
    "        # Concatenate the current style DataFrame to the empty_df_var DataFrame to add it to the new DataFrame\n",
    "        fin_df = pd.concat([empty_df_var, style_df], ignore_index=True)\n",
    "\n",
    "        # Update the empty_df_var DataFrame with the new DataFrame containing the current style\n",
    "        empty_df_var = fin_df\n",
    "\n",
    "    # Return the final DataFrame containing the balanced samples for all styles\n",
    "    return fin_df\n",
    "\n",
    "\n",
    "df = style_balancer(overpop_styles, df)\n",
    "df[\"style\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING SHI OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_returner(img_name):\n",
    "    for i in range(len(df)):\n",
    "        if img_name == df.loc[i, \"new_filename\"]:\n",
    "            return df.loc[i, \"style\"]\n",
    "        \n",
    "style_returner('1165.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHUFFLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df[\"in_train\"].value_counts()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define data generators for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create a flow_from_dataframe generator for the training data\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=train_1_folder,\n",
    "    x_col='new_filename',\n",
    "    y_col='style',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=train_1_folder,\n",
    "    x_col='new_filename',\n",
    "    y_col='style',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of samples for each class in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = train_generator.class_indices\n",
    "classes = train_generator.classes\n",
    "\n",
    "newd = {key: 0 for key in class_indices}\n",
    "\n",
    "for i in classes:\n",
    "    for j, k in class_indices.items():\n",
    "        if i == k:\n",
    "            newd[j] += 1\n",
    "\n",
    "newd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of samples for each class in testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = test_generator.class_indices\n",
    "classes = test_generator.classes\n",
    "\n",
    "newd = {key: 0 for key in class_indices}\n",
    "\n",
    "for i in classes:\n",
    "    for j, k in class_indices.items():\n",
    "        if i == k:\n",
    "            newd[j] += 1\n",
    "\n",
    "newd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator)\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels and predicted labels for the test data\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(model.predict(test_generator), axis=-1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(test_generator.class_indices))\n",
    "plt.xticks(tick_marks, test_generator.class_indices, rotation=90)\n",
    "plt.yticks(tick_marks, test_generator.class_indices)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(14, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator)\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(model.predict(test_generator), axis=-1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(test_generator.class_indices))\n",
    "plt.xticks(tick_marks, test_generator.class_indices, rotation=90)\n",
    "plt.yticks(tick_marks, test_generator.class_indices)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "19dc0638c6a44b1d782d2afa44ecf5e378f214d4a310d51586413002f0824043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
